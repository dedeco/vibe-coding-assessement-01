# Data Ingestion & Deployment Guide

This document explains how to populate the vector database for different deployment scenarios.

## ğŸ¯ TL;DR - For POC Deployment

**Fastest approach for demonstration:**

```bash
# Build database with test data and start API
python scripts/build_database.py --source test --reset
cd src/web && python app.py
```

## ğŸ“Š Data Ingestion Options

### Option 1: Test Data (Recommended for POC)

**Best for:** Demonstrations, development, POC deployments

```bash
# Generate diverse test data and populate ChromaDB
python scripts/build_database.py --source test --reset
```

**Advantages:**
- âœ… No external dependencies
- âœ… Diverse, realistic data (547 documents)
- âœ… Fast build time (~30 seconds)
- âœ… Predictable, testable responses
- âœ… No sensitive data concerns

### Option 2: PDF Files (Production)

**Best for:** Real deployments with actual trial balance PDFs

```bash
# Process PDFs and populate ChromaDB
python scripts/build_database.py --source pdf --pdf-path pdfs --reset
```

**Requirements:**
- PDF files in the specified folder
- Files should follow naming pattern: `PACTO_BALANCETE_0913_YYMM.pdf`
- PDFs contain structured trial balance data

## ğŸš€ Deployment Strategies

### 1. Build-Time Population (Recommended for POC)

Populate database during application build/startup:

```bash
# Method A: Using build script
./scripts/build_and_run.sh

# Method B: Manual steps  
python scripts/build_database.py --source test --reset
cd src/web
python app.py
```

### 2. Docker Build-Time

Add to your `Dockerfile`:

```dockerfile
# Copy build files
COPY scripts/build_database.py /app/
COPY scripts/populate_test_data.py /app/
COPY src/ /app/src/

# Build database during image creation
RUN python scripts/build_database.py --source test --reset

# Database ready when container starts
CMD ["python", "src/web/app.py"]
```

### 3. Init Container (Kubernetes)

```yaml
apiVersion: v1
kind: Pod
spec:
  initContainers:
  - name: database-builder
    image: your-app:latest
    command: ['python', 'build_database.py', '--source', 'test', '--reset']
    volumeMounts:
    - name: database-volume
      mountPath: /app/src/web/data
  
  containers:
  - name: api-server
    image: your-app:latest
    volumeMounts:
    - name: database-volume  
      mountPath: /app/src/web/data
```

### 4. Runtime Population

For dynamic data loading:

```python
# In your app startup
from build_database import DatabaseBuilder

@app.on_event("startup")
async def populate_database():
    if not database_exists():
        builder = DatabaseBuilder()
        builder.build_from_test_data(reset=True)
```

## ğŸ—‚ï¸ Data Pipeline Architecture

```
PDF Files OR Test Data
         â†“
   PDF Processor / Generator
         â†“  
   Structured Expenses
         â†“
   Semantic Chunker
         â†“
   Document Chunks
         â†“
   ChromaDB Indexer
         â†“
   Vector Database
         â†“
   API Ready! ğŸ‰
```

## ğŸ“ File Structure

```
project/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_database.py          # Main build script
â”‚   â”œâ”€â”€ populate_test_data.py       # Test data generator  
â”œâ”€â”€ pdfs/                       # PDF source files
â”‚   â”œâ”€â”€ PACTO_BALANCETE_*.pdf
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ pdf_processor.py    # PDF â†’ structured data
â”‚   â”‚   â”œâ”€â”€ semantic_chunker.py # Structure â†’ chunks
â”‚   â”‚   â””â”€â”€ indexer.py          # Chunks â†’ vector DB
â”‚   â””â”€â”€ web/
â”‚       â”œâ”€â”€ data/chromadb/      # Vector database
â”‚       â””â”€â”€ app.py              # API server
â””â”€â”€ data/
    â”œâ”€â”€ processed/              # Intermediate files
    â””â”€â”€ chunks/                 # Semantic chunks
```

## âš™ï¸ Configuration Options

### Environment Variables

```bash
# Optional: Claude API for enhanced responses
export CLAUDE_API_KEY="your-api-key"

# Database path (default: src/web/data/chromadb)
export DB_PATH="custom/path"

# API port (default: 8000)  
export PORT=8080
```

### Build Script Options

```bash
# Reset database (clear existing data)
python scripts/build_database.py --reset

# Choose data source
python scripts/build_database.py --source pdf    # Use PDF files
python scripts/build_database.py --source test   # Use test data

# Custom PDF path
python scripts/build_database.py --source pdf --pdf-path /custom/path

# Create deployment scripts
python scripts/build_database.py --create-scripts
```

## ğŸ§ª Testing & Verification

After building the database:

```bash
# Verify database population
python scripts/build_database.py --source test  # Will verify at the end

# Test API responses
python scripts/test_api_responses.py

# Manual API test
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "How much was spent on electricity?"}'
```

## ğŸ“ˆ Performance & Scale

### POC/Demo (Test Data)
- **Size:** 547 documents, ~2MB
- **Build time:** 30 seconds
- **Memory:** ~100MB
- **Response time:** <200ms

### Production (PDF Data)
- **Size:** Depends on PDF count
- **Build time:** ~1-5 minutes for 50 PDFs
- **Memory:** ~500MB-2GB
- **Response time:** <500ms

## ğŸ”§ Troubleshooting

### Common Issues

**Database empty after build:**
```bash
# Check database location
ls -la src/web/data/chromadb/

# Verify with explicit path
python -c "
from src.query.retriever import ExpenseRetriever
r = ExpenseRetriever('src/web/data/chromadb')
print(f'Documents: {r.get_collection().count()}')
"
```

**Import errors:**
```bash
# Ensure Python path includes src/
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"

# Or use absolute imports
python -c "import sys; sys.path.append('src'); from build_database import DatabaseBuilder"
```

**ChromaDB permission errors:**
```bash
# Fix permissions
chmod -R 755 src/web/data/chromadb/
```

## ğŸ’¡ Best Practices

### For POC/Demo:
1. âœ… Use test data (`--source test`)
2. âœ… Build at container build time
3. âœ… Include verification step
4. âœ… Add health check endpoints

### For Production:
1. âœ… Use actual PDF files (`--source pdf`)
2. âœ… Implement incremental updates
3. âœ… Add data validation
4. âœ… Monitor database size/performance
5. âœ… Backup vector database regularly

### General:
1. âœ… Always verify after build
2. âœ… Test with diverse queries
3. âœ… Monitor response times
4. âœ… Log build process for debugging